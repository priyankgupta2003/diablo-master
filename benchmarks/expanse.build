#!/bin/bash
#SBATCH -A uot166
#SBATCH --job-name="diablo"
#SBATCH --output="run.log"
#SBATCH --partition=compute
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --mem=16G
#SBATCH --export=ALL
#SBATCH --time=10    # time limit in minutes

module load openjdk

SW=/expanse/lustre/projects/uot166/fegaras

export DIABLO_HOME=$SW/diablo
export SCALA_HOME=$SW/scala-2.12.3
export SPARK_HOME=$SW/spark-3.1.2-bin-hadoop3.2

PATH="$SPARK_HOME/bin:$SCALA_HOME/bin:$PATH"

JARS=.
for I in `ls $SPARK_HOME/jars/*.jar -I *unsafe*`; do
    JARS=$JARS:$I
done

rm -rf classes
mkdir -p classes

scala_files="add.scala factorization.scala linear_regression.scala multiply.scala nn_binary_classification.scala pagerank.scala"
for f in $scala_files; do
    echo compiling $f ...
    scalac -d classes -cp classes:${JARS}:${DIABLO_HOME}/lib/diablo.jar $f >/dev/null
done
echo compilation complete...
jar cf test.jar -C classes .
